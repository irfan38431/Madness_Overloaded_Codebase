{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f10e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGBM = True\n",
    "except Exception:\n",
    "    HAS_LGBM = False\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import gc\n",
    "from functools import lru_cache\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c076d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "class CFG:\n",
    "    # Data paths: update to where your CSVs are\n",
    "    ORDER_DATA_PATH = r\"C:\\Users\\ia383\\Madness_Overloaded_WWT_Comp2025\\Madness_Overloaded_Codebase\\dataset\\order_data.csv\"\n",
    "    CUSTOMER_DATA_PATH = r\"C:\\Users\\ia383\\Madness_Overloaded_WWT_Comp2025\\Madness_Overloaded_Codebase\\dataset\\customer_data.csv\"  \n",
    "    STORE_DATA_PATH = r\"C:\\Users\\ia383\\Madness_Overloaded_WWT_Comp2025\\Madness_Overloaded_Codebase\\dataset\\store_data.csv\"        \n",
    "    TEST_DATA_PATH = r\"C:\\Users\\ia383\\Madness_Overloaded_WWT_Comp2025\\Madness_Overloaded_Codebase\\dataset\\test_data_question.csv\"\n",
    "\n",
    "    OUTPUT_EXCEL_PATH = r\"C:\\Users\\ia383\\Madness_Overloaded_WWT_Comp2025\\TEAM_WWT_Comp2025_Output_fast.xlsx\"\n",
    "    SEED = 42\n",
    "\n",
    "    # Catalog / sample controls\n",
    "    MAX_ITEMS = 3000          # cap on unique items kept (most-frequent)\n",
    "    MIN_ITEMS_PER_CART = 2\n",
    "    SAMPLE_RECENT_DAYS = None # if int, only keep orders within N most recent days\n",
    "    MAX_TRAIN_SAMPLES = 400000  # cap supervised samples (None to disable)\n",
    "\n",
    "    # Co-occurrence / neighbors\n",
    "    TOP_NEIGHBORS = 50\n",
    "    ALPHA = 0.75              # weight down large carts\n",
    "    RECENCY_HALF_LIFE_DAYS = 180  # decays older cooccurrence counts\n",
    "\n",
    "    # Candidate & scoring\n",
    "    TOP_POPULAR_K = 40\n",
    "    CAND_LIMIT = 200\n",
    "    RECALL_AT_K = 3\n",
    "    WEIGHT_COOCC = 0.7\n",
    "    WEIGHT_POP = 0.25\n",
    "    WEIGHT_EMB = 0.05         # used only if embeddings available\n",
    "\n",
    "    # Word2Vec (optional)\n",
    "    USE_W2V = False and HAS_W2V\n",
    "    W2V_SIZE = 64\n",
    "    W2V_WINDOW = 5\n",
    "    W2V_EPOCHS = 5\n",
    "\n",
    "# deterministic\n",
    "random.seed(CFG.SEED)\n",
    "np.random.seed(CFG.SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa0044",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a7213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stamp(msg: str):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed6b8ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_read_csv(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    stamp(f\"Reading: {path}\")\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83ca2eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_orders_cell(orders_cell):\n",
    "    \"\"\"Robust parsing of ORDERS into a list of item names.\"\"\"\n",
    "    if pd.isna(orders_cell):\n",
    "        return []\n",
    "    if isinstance(orders_cell, list):\n",
    "        return [str(x).strip() for x in orders_cell]\n",
    "    s = str(orders_cell)\n",
    "    # try json then literal\n",
    "    for fn in (json.loads, ast.literal_eval):\n",
    "        try:\n",
    "            obj = fn(s)\n",
    "            break\n",
    "        except Exception:\n",
    "            obj = None\n",
    "    if obj is None:\n",
    "        return []\n",
    "    items = []\n",
    "    if isinstance(obj, dict) and \"orders\" in obj:\n",
    "        for ord_entry in obj.get(\"orders\", []):\n",
    "            details = ord_entry.get(\"item_details\", [])\n",
    "            for d in details:\n",
    "                name = d.get(\"item_name\")\n",
    "                if name:\n",
    "                    items.append(str(name).strip())\n",
    "    elif isinstance(obj, list):\n",
    "        items = [str(x).strip() for x in obj if isinstance(x, (str, int))]\n",
    "    else:\n",
    "        # fallback: split by comma (rare)\n",
    "        items = [part.strip() for part in s.split(\",\") if part.strip()]\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b05ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_orders_to_lists(df):\n",
    "    stamp(\"Parsing ORDERS -> ITEM_LIST\")\n",
    "    return df[\"ORDERS\"].apply(parse_orders_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a20e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_order_date(s):\n",
    "    # try multiple date formats commonly found; return pandas Timestamp or NaT\n",
    "    if pd.isna(s):\n",
    "        return pd.NaT\n",
    "    if isinstance(s, (pd.Timestamp, datetime)):\n",
    "        return pd.to_datetime(s)\n",
    "    for fmt in (\"%Y-%m-%d %H:%M:%S\", \"%Y-%m-%d\", \"%d-%m-%Y %H:%M:%S\",\n",
    "                \"%d-%m-%Y\", \"%m/%d/%Y %H:%M:%S\", \"%m/%d/%Y\"):\n",
    "        try:\n",
    "            return pd.to_datetime(s, format=fmt)\n",
    "        except Exception:\n",
    "            continue\n",
    "    # fallback auto parse\n",
    "    try:\n",
    "        return pd.to_datetime(s, errors=\"coerce\")\n",
    "    except Exception:\n",
    "        return pd.NaT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c28981",
   "metadata": {},
   "source": [
    "# # Build catalog + cooccurrence Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60f500e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_catalog_and_filtered_orders(order_df):\n",
    "    stamp(\"Building item counts and applying catalog cap...\")\n",
    "    order_df = order_df.copy()\n",
    "    order_df[\"ITEM_LIST\"] = explode_orders_to_lists(order_df)\n",
    "    # optional: remove carts with 0 items\n",
    "    order_df = order_df[order_df[\"ITEM_LIST\"].map(len) > 0].reset_index(drop=True)\n",
    "    # parse dates\n",
    "    if \"ORDER_CREATED_DATE\" in order_df.columns:\n",
    "        order_df[\"_ORD_DATE\"] = order_df[\"ORDER_CREATED_DATE\"].apply(parse_order_date)\n",
    "        # optional filter recent\n",
    "        if isinstance(CFG.SAMPLE_RECENT_DAYS, int):\n",
    "            max_date = order_df[\"_ORD_DATE\"].max()\n",
    "            cutoff = max_date - pd.Timedelta(days=CFG.SAMPLE_RECENT_DAYS)\n",
    "            order_df = order_df[order_df[\"_ORD_DATE\"] >= cutoff].reset_index(drop=True)\n",
    "    else:\n",
    "        order_df[\"_ORD_DATE\"] = pd.NaT\n",
    "\n",
    "    # item frequency\n",
    "    ctr = Counter()\n",
    "    for lst in order_df[\"ITEM_LIST\"]:\n",
    "        ctr.update([it for it in lst])\n",
    "\n",
    "    most_common_items = [it for it, _ in ctr.most_common(CFG.MAX_ITEMS)]\n",
    "    kept_set = set(most_common_items)\n",
    "\n",
    "    # drop items not in top-K and reduce large carts\n",
    "    def filter_cart(lst):\n",
    "        filt = [it for it in lst if it in kept_set]\n",
    "        # dedupe keeping order\n",
    "        seen = set()\n",
    "        out = []\n",
    "        for it in filt:\n",
    "            if it not in seen:\n",
    "                seen.add(it)\n",
    "                out.append(it)\n",
    "        return out\n",
    "\n",
    "    order_df[\"ITEM_LIST\"] = order_df[\"ITEM_LIST\"].apply(filter_cart)\n",
    "    order_df = order_df[order_df[\"ITEM_LIST\"].map(len) >= CFG.MIN_ITEMS_PER_CART].reset_index(drop=True)\n",
    "    # recompute counts over filtered\n",
    "    filtered_ctr = Counter()\n",
    "    for lst in order_df[\"ITEM_LIST\"]:\n",
    "        filtered_ctr.update(lst)\n",
    "\n",
    "    item2idx = {it: i for i, it in enumerate(filtered_ctr.keys())}\n",
    "    idx2item = {i: it for it, i in item2idx.items()}\n",
    "\n",
    "    stamp(f\"Orders retained: {len(order_df)}, unique items kept: {len(item2idx)}\")\n",
    "    return order_df, item2idx, idx2item, filtered_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0238bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recency_weighted_factor(order_date, newest_date):\n",
    "    \"\"\"Return weight in (0,1] for an order based on recency; half-life controlled by CFG.RECENCY_HALF_LIFE_DAYS.\"\"\"\n",
    "    if pd.isna(order_date) or pd.isna(newest_date):\n",
    "        return 1.0\n",
    "    days = (newest_date - order_date).days\n",
    "    if days <= 0:\n",
    "        return 1.0\n",
    "    half = CFG.RECENCY_HALF_LIFE_DAYS\n",
    "    # exponential decay: weight = 0.5^(days/half)\n",
    "    return 0.5 ** (days / max(1.0, half))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b49a630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cooccurrence_neighbors(order_df, item2idx):\n",
    "    stamp(\"Computing recency-weighted co-occurrence counts (dict-of-dicts)...\")\n",
    "    newest = order_df[\"_ORD_DATE\"].max() if \"_ORD_DATE\" in order_df.columns else pd.NaT\n",
    "    co_counts = defaultdict(lambda: defaultdict(float))\n",
    "    # accumulate symmetric counts\n",
    "    for lst, d in zip(order_df[\"ITEM_LIST\"], order_df[\"_ORD_DATE\"]):\n",
    "        uniq = list(dict.fromkeys(lst))\n",
    "        if len(uniq) <= 1:\n",
    "            continue\n",
    "        w = 1.0 / (len(uniq) ** CFG.ALPHA)\n",
    "        rec_w = recency_weighted_factor(d, newest)\n",
    "        total_w = w * rec_w\n",
    "        for i in range(len(uniq)):\n",
    "            a = uniq[i]\n",
    "            for j in range(i + 1, len(uniq)):\n",
    "                b = uniq[j]\n",
    "                co_counts[a][b] += total_w\n",
    "                co_counts[b][a] += total_w\n",
    "\n",
    "    # convert each item's co-dict to top-K neighbor list (sorted)\n",
    "    neighbors = {}\n",
    "    for it, nbrs in co_counts.items():\n",
    "        # sort by descending weight\n",
    "        sorted_n = sorted(nbrs.items(), key=lambda x: -x[1])[:CFG.TOP_NEIGHBORS]\n",
    "        neighbors[it] = [(n, float(score)) for n, score in sorted_n]\n",
    "    stamp(f\"Built neighbor lists for {len(neighbors)} items\")\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f608df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_popularity_tables(order_df):\n",
    "    stamp(\"Building popularity (global & store-wise & time-of-day)...\")\n",
    "    global_ctr = Counter()\n",
    "    store_ctr = defaultdict(Counter)\n",
    "    hour_ctr = defaultdict(Counter)  # hour -> Counter\n",
    "    for lst, store, dt in zip(order_df[\"ITEM_LIST\"],\n",
    "                              order_df.get(\"STORE_NUMBER\", [None]*len(order_df)),\n",
    "                              order_df[\"_ORD_DATE\"]):\n",
    "        global_ctr.update(lst)\n",
    "        store_ctr[store].update(lst)\n",
    "        if not pd.isna(dt):\n",
    "            hour_ctr[dt.hour].update(lst)\n",
    "    # lists\n",
    "    global_pop = [it for it, _ in global_ctr.most_common()]\n",
    "    store_pop = {s: [it for it, _ in c.most_common()] for s, c in store_ctr.items()}\n",
    "    hour_pop = {h: [it for it, _ in c.most_common()] for h, c in hour_ctr.items()}\n",
    "    return global_pop, store_pop, hour_pop, global_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fc0c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_item2vec(order_df):\n",
    "    if not CFG.USE_W2V:\n",
    "        return None\n",
    "    if not HAS_W2V:\n",
    "        stamp(\"[WARN] gensim not available; skipping Word2Vec.\")\n",
    "        return None\n",
    "    stamp(\"Training Word2Vec on carts...\")\n",
    "    sentences = [lst for lst in order_df[\"ITEM_LIST\"] if len(lst) >= 1]\n",
    "    model = Word2Vec(sentences=sentences,\n",
    "                     vector_size=CFG.W2V_SIZE,\n",
    "                     window=CFG.W2V_WINDOW,\n",
    "                     min_count=1,\n",
    "                     workers=2,\n",
    "                     epochs=CFG.W2V_EPOCHS,\n",
    "                     seed=CFG.SEED)\n",
    "    stamp(\"Word2Vec trained.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42add549",
   "metadata": {},
   "source": [
    "#  Candidate generation + scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6bcab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates_for_cart(cart, neighbors, global_pop, store_pop=None, store=None, hour_pop=None, hour=None):\n",
    "    cand = []\n",
    "    cand_set = set()\n",
    "    # 1) neighbors\n",
    "    for it in cart:\n",
    "        for n, score in neighbors.get(it, []):\n",
    "            if n not in cand_set and n not in cart:\n",
    "                cand_set.add(n)\n",
    "                cand.append(('nbr', n, score))\n",
    "    # 2) store pop / hour pop prioritized\n",
    "    if store is not None and store_pop and store in store_pop:\n",
    "        for it in store_pop[store][:CFG.TOP_POPULAR_K]:\n",
    "            if it not in cand_set and it not in cart:\n",
    "                cand_set.add(it)\n",
    "                cand.append(('storepop', it, None))\n",
    "    if hour is not None and hour_pop and hour in hour_pop:\n",
    "        for it in hour_pop[hour][:CFG.TOP_POPULAR_K]:\n",
    "            if it not in cand_set and it not in cart:\n",
    "                cand_set.add(it)\n",
    "                cand.append(('hourpop', it, None))\n",
    "    # 3) global pop fallback\n",
    "    for it in global_pop[:CFG.TOP_POPULAR_K]:\n",
    "        if it not in cand_set and it not in cart:\n",
    "            cand_set.add(it)\n",
    "            cand.append(('gpop', it, None))\n",
    "    # limit\n",
    "    return cand[:CFG.CAND_LIMIT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4a07f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(cart, candidates, neighbors, global_ctr, w_co, w_pop, emb_model=None):\n",
    "    \"\"\"Return list of (item, score) sorted desc. candidates: list of tuples (source, item, co_score_or_None)\"\"\"\n",
    "    scores = []\n",
    "    # precompute co-sum from cart to candidate (fast lookup in neighbors dict)\n",
    "    # Build dict of co-sums per candidate\n",
    "    cand_set = [it for (_, it, _) in candidates]\n",
    "    co_sum = {c: 0.0 for c in cand_set}\n",
    "    for it in cart:\n",
    "        nbrs = dict(neighbors.get(it, []))\n",
    "        for c in cand_set:\n",
    "            co_sum[c] += float(nbrs.get(c, 0.0))\n",
    "\n",
    "    # popularity scores normalized by global_ctr max\n",
    "    max_pop = max(global_ctr.values()) if global_ctr else 1\n",
    "    for (_, item, co_hint) in candidates:\n",
    "        co_s = co_sum.get(item, 0.0)\n",
    "        pop_s = float(global_ctr.get(item, 0)) / max_pop\n",
    "        emb_s = 0.0\n",
    "        if emb_model is not None and HAS_W2V:\n",
    "            # average similarity between item and cart items if present\n",
    "            try:\n",
    "                vec_item = emb_model.wv[item]\n",
    "                sims = []\n",
    "                for it in cart:\n",
    "                    if it in emb_model.wv:\n",
    "                        sims.append(np.dot(vec_item, emb_model.wv[it]) / (\n",
    "                            np.linalg.norm(vec_item) * np.linalg.norm(emb_model.wv[it]) + 1e-9))\n",
    "                if sims:\n",
    "                    emb_s = float(np.mean(sims))\n",
    "            except Exception:\n",
    "                emb_s = 0.0\n",
    "        score = w_co * co_s + w_pop * pop_s + CFG.WEIGHT_EMB * emb_s\n",
    "        scores.append((item, score))\n",
    "    scores.sort(key=lambda x: -x[1])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9124687d",
   "metadata": {},
   "source": [
    "# Generating Training Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b17bbff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_out_samples(order_df):\n",
    "    stamp(\"Generating leave-one-out samples for offline eval...\")\n",
    "    rows = []\n",
    "    for lst, store, dt in zip(order_df[\"ITEM_LIST\"], order_df.get(\"STORE_NUMBER\", [None]*len(order_df)), order_df[\"_ORD_DATE\"]):\n",
    "        uniq = list(dict.fromkeys(lst))\n",
    "        if len(uniq) < CFG.MIN_ITEMS_PER_CART:\n",
    "            continue\n",
    "        for i, target in enumerate(uniq):\n",
    "            left = [x for j,x in enumerate(uniq) if j!=i]\n",
    "            rows.append({\"features_items\": left, \"target_item\": target, \"STORE_NUMBER\": store, \"_ORD_DATE\": dt})\n",
    "    if CFG.MAX_TRAIN_SAMPLES and len(rows) > CFG.MAX_TRAIN_SAMPLES:\n",
    "        stamp(f\"Sampling down supervised rows from {len(rows)} to {CFG.MAX_TRAIN_SAMPLES}\")\n",
    "        rows = random.sample(rows, CFG.MAX_TRAIN_SAMPLES)\n",
    "    df = pd.DataFrame(rows)\n",
    "    stamp(f\"Total leave-one-out samples: {len(df)}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebeb95b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall_at_k_fast(samples_df, neighbors, global_pop, global_ctr, store_pop=None, hour_pop=None, emb_model=None, K=3):\n",
    "    stamp(\"Computing Recall@K (fast, candidate-limited)...\")\n",
    "    n = len(samples_df)\n",
    "    correct = 0\n",
    "    start = time.time()\n",
    "    for i, row in samples_df.iterrows():\n",
    "        cart = row[\"features_items\"]\n",
    "        target = row[\"target_item\"]\n",
    "        store = row.get(\"STORE_NUMBER\", None)\n",
    "        dt = row.get(\"_ORD_DATE\", pd.NaT)\n",
    "        hour = dt.hour if not pd.isna(dt) else None\n",
    "\n",
    "        cand = candidates_for_cart(cart, neighbors, global_pop, store_pop, store, hour_pop, hour)\n",
    "        scored = compute_scores(cart, cand, neighbors, global_ctr, CFG.WEIGHT_COOCC, CFG.WEIGHT_POP, emb_model)\n",
    "        preds = [it for it, _ in scored][:K]\n",
    "        if target in preds:\n",
    "            correct += 1\n",
    "\n",
    "        if (i+1) % 5000 == 0 or i+1 == n:\n",
    "            elapsed = time.time() - start\n",
    "            stamp(f\"[val] {i+1}/{n} | interim Recall@{K}={correct/(i+1):.4f} | {i+1:.1f}/{elapsed:.1f}s\")\n",
    "\n",
    "    recall = correct / max(1, n)\n",
    "    stamp(f\"[METRIC] Recall@{K}: {recall:.4f} (time {(time.time()-start)/60:.2f} min)\")\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3cd3cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_on_test(test_df, neighbors, global_pop, global_ctr, store_pop=None, hour_pop=None, emb_model=None):\n",
    "    stamp(\"Running inference on test sheet...\")\n",
    "    cart_item_cols = [c for c in [\"item1\", \"item2\", \"item3\", \"item4\"] if c in test_df.columns]\n",
    "    out_rows = []\n",
    "    for r, row in test_df.iterrows():\n",
    "        cart = []\n",
    "        for c in cart_item_cols:\n",
    "            it = row.get(c, None)\n",
    "            if pd.notna(it) and str(it).strip().lower() != \"missing\":\n",
    "                cart.append(str(it).strip())\n",
    "        cart = list(dict.fromkeys(cart))\n",
    "        store = row.get(\"STORE_NUMBER\", None) if \"STORE_NUMBER\" in test_df.columns else None\n",
    "        dt = row.get(\"ORDER_CREATED_DATE\", None)\n",
    "        hour = None\n",
    "        if dt is not None and not pd.isna(dt):\n",
    "            try:\n",
    "                hour = parse_order_date(dt).hour\n",
    "            except Exception:\n",
    "                hour = None\n",
    "\n",
    "        cand = candidates_for_cart(cart, neighbors, global_pop, store_pop, store, hour_pop, hour)\n",
    "        scored = compute_scores(cart, cand, neighbors, global_ctr, CFG.WEIGHT_COOCC, CFG.WEIGHT_POP, emb_model)\n",
    "        preds = [it for it, _ in scored][:CFG.RECALL_AT_K]\n",
    "        # ensure length\n",
    "        while len(preds) < CFG.RECALL_AT_K:\n",
    "            for it in global_pop:\n",
    "                if it not in cart and it not in preds:\n",
    "                    preds.append(it)\n",
    "                if len(preds) >= CFG.RECALL_AT_K:\n",
    "                    break\n",
    "\n",
    "        out = {\n",
    "            \"CUSTOMER_ID\": row.get(\"CUSTOMER_ID\", \"\"),\n",
    "            \"ORDER_ID\": row.get(\"ORDER_ID\", \"\")\n",
    "        }\n",
    "        for c in cart_item_cols:\n",
    "            out[c] = row.get(c, \"\")\n",
    "        for i in range(CFG.RECALL_AT_K):\n",
    "            out[f\"RECOMMENDATION {i+1}\"] = preds[i] if i < len(preds) else \"\"\n",
    "        out_rows.append(out)\n",
    "    out_df = pd.DataFrame(out_rows)\n",
    "    cols = [\"CUSTOMER_ID\", \"ORDER_ID\"] + cart_item_cols + [f\"RECOMMENDATION {i+1}\" for i in range(CFG.RECALL_AT_K)]\n",
    "    cols = [c for c in cols if c in out_df.columns]\n",
    "    out_df = out_df[cols]\n",
    "    out_df.to_excel(CFG.OUTPUT_EXCEL_PATH, index=False)\n",
    "    stamp(f\"Wrote output Excel -> {CFG.OUTPUT_EXCEL_PATH}\")\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "add2b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline():\n",
    "    stamp(\"=== FAST PIPELINE START ===\")\n",
    "    # 1) Load data\n",
    "    order_df = safe_read_csv(CFG.ORDER_DATA_PATH)\n",
    "    # optional merges if you want but not required to run baseline\n",
    "    # 2) Filter and build catalog\n",
    "    order_df, item2idx, idx2item, filtered_ctr = build_catalog_and_filtered_orders(order_df)\n",
    "    # 3) neighbors & pop tables\n",
    "    neighbors = build_cooccurrence_neighbors(order_df, item2idx)\n",
    "    global_pop, store_pop, hour_pop, global_ctr = build_popularity_tables(order_df)\n",
    "    # 4) optional embeddings\n",
    "    emb_model = None\n",
    "    if CFG.USE_W2V:\n",
    "        emb_model = train_item2vec(order_df)\n",
    "\n",
    "    # 5) build leave-one-out samples and evaluate\n",
    "    samples = leave_one_out_samples(order_df)\n",
    "    # quick shuffle\n",
    "    samples = samples.sample(frac=1.0, random_state=CFG.SEED).reset_index(drop=True)\n",
    "    # evaluate on a holdout slice (fast)\n",
    "    n_val = int(0.15 * len(samples)) if len(samples) > 1000 else min(2000, len(samples))\n",
    "    val_df = samples.iloc[:n_val]\n",
    "    stamp(f\"Using {len(val_df)} samples for quick offline validation\")\n",
    "    recall = compute_recall_at_k_fast(val_df, neighbors, global_pop, global_ctr, store_pop, hour_pop, emb_model, K=CFG.RECALL_AT_K)\n",
    "\n",
    "    # 6) inference on test sheet and write excel\n",
    "    test_df = safe_read_csv(CFG.TEST_DATA_PATH)\n",
    "    out_df = run_inference_on_test(test_df, neighbors, global_pop, global_ctr, store_pop, hour_pop, emb_model)\n",
    "\n",
    "    stamp(\"=== FAST PIPELINE END ===\")\n",
    "    return {\"val_recall3\": float(recall), \"output_path\": CFG.OUTPUT_EXCEL_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e21547ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:12:42] === FAST PIPELINE START ===\n",
      "[22:12:42] Reading: C:\\Users\\ia383\\Madness_Overloaded_WWT_Comp2025\\Madness_Overloaded_Codebase\\dataset\\order_data.csv\n",
      "[22:12:47] Building item counts and applying catalog cap...\n",
      "[22:12:47] Parsing ORDERS -> ITEM_LIST\n",
      "[22:16:34] Orders retained: 1414398, unique items kept: 145\n",
      "[22:16:34] Computing recency-weighted co-occurrence counts (dict-of-dicts)...\n",
      "[22:16:45] Built neighbor lists for 145 items\n",
      "[22:16:45] Building popularity (global & store-wise & time-of-day)...\n",
      "[22:16:52] Generating leave-one-out samples for offline eval...\n",
      "[22:17:25] Sampling down supervised rows from 5656980 to 400000\n",
      "[22:17:29] Total leave-one-out samples: 400000\n",
      "[22:17:29] Using 60000 samples for quick offline validation\n",
      "[22:17:29] Computing Recall@K (fast, candidate-limited)...\n",
      "[22:17:31] [val] 5000/60000 | interim Recall@3=0.6086 | 5000.0/1.2s\n",
      "[22:17:32] [val] 10000/60000 | interim Recall@3=0.6200 | 10000.0/2.4s\n",
      "[22:17:33] [val] 15000/60000 | interim Recall@3=0.6206 | 15000.0/3.7s\n",
      "[22:17:34] [val] 20000/60000 | interim Recall@3=0.6177 | 20000.0/4.9s\n",
      "[22:17:36] [val] 25000/60000 | interim Recall@3=0.6183 | 25000.0/6.2s\n",
      "[22:17:37] [val] 30000/60000 | interim Recall@3=0.6184 | 30000.0/7.4s\n",
      "[22:17:38] [val] 35000/60000 | interim Recall@3=0.6206 | 35000.0/8.5s\n",
      "[22:17:39] [val] 40000/60000 | interim Recall@3=0.6193 | 40000.0/9.6s\n",
      "[22:17:40] [val] 45000/60000 | interim Recall@3=0.6193 | 45000.0/10.8s\n",
      "[22:17:42] [val] 50000/60000 | interim Recall@3=0.6193 | 50000.0/12.1s\n",
      "[22:17:43] [val] 55000/60000 | interim Recall@3=0.6188 | 55000.0/13.2s\n",
      "[22:17:44] [val] 60000/60000 | interim Recall@3=0.6186 | 60000.0/14.4s\n",
      "[22:17:44] [METRIC] Recall@3: 0.6186 (time 0.24 min)\n",
      "[22:17:44] Reading: C:\\Users\\ia383\\Madness_Overloaded_WWT_Comp2025\\Madness_Overloaded_Codebase\\dataset\\test_data_question.csv\n",
      "[22:17:44] Running inference on test sheet...\n",
      "[22:17:44] Wrote output Excel -> C:\\Users\\ia383\\Madness_Overloaded_WWT_Comp2025\\TEAM_WWT_Comp2025_Output_fast.xlsx\n",
      "[22:17:44] === FAST PIPELINE END ===\n",
      "[22:17:45] Final offline Recall@3: 0.6186\n",
      "[22:17:45] Output: C:\\Users\\ia383\\Madness_Overloaded_WWT_Comp2025\\TEAM_WWT_Comp2025_Output_fast.xlsx\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    res = run_pipeline()\n",
    "    stamp(f\"Final offline Recall@{CFG.RECALL_AT_K}: {res['val_recall3']:.4f}\")\n",
    "    stamp(f\"Output: {res['output_path']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e893e413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ia383\\Madness_Overloaded_WWT_Comp2025\\Madness_Overloaded_Codebase\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())  # Shows current working directory\n",
    "print(os.path.exists(r\"C:\\Users\\ia383\\Madness_Overloaded_WWT_Comp2025\\Madness_Overloaded_Codebase\\dataset\\order_data.csv\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
